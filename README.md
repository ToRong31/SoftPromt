# T5 Prompt Tuning (PEFT) cho PhÃ¢n Loáº¡i Äa NhÃ£n

## ğŸ“– Giá»›i thiá»‡u

Project nÃ y sá»­ dá»¥ng mÃ´ hÃ¬nh **T5-base** vá»›i ká»¹ thuáº­t **Prompt Tuning (PEFT - Parameter Efficient Fine-Tuning)** Ä‘á»ƒ giáº£i quyáº¿t bÃ i toÃ¡n phÃ¢n loáº¡i Ä‘a nhÃ£n (multi-label classification) dÆ°á»›i dáº¡ng cÃ¢u há»i **Yes/No**.

### CÃ¡ch hoáº¡t Ä‘á»™ng

Thay vÃ¬ fine-tune toÃ n bá»™ tham sá»‘ cá»§a mÃ´ hÃ¬nh T5, Prompt Tuning chá»‰ huáº¥n luyá»‡n má»™t sá»‘ lÆ°á»£ng nhá» "virtual tokens" (tokens áº£o) Ä‘Æ°á»£c thÃªm vÃ o Ä‘áº§u input. Äiá»u nÃ y giÃºp:
- Giáº£m Ä‘Ã¡ng ká»ƒ sá»‘ lÆ°á»£ng tham sá»‘ cáº§n huáº¥n luyá»‡n
- Tiáº¿t kiá»‡m bá»™ nhá»› vÃ  thá»i gian training
- Dá»… dÃ ng lÆ°u trá»¯ vÃ  chia sáº» mÃ´ hÃ¬nh (chá»‰ cáº§n lÆ°u adapter)

Má»—i máº«u dá»¯ liá»‡u sáº½ Ä‘Æ°á»£c chuyá»ƒn thÃ nh prompt dáº¡ng:

```
{comment}

Is the text above {label}?
```

Model sáº½ tráº£ lá»i **"yes"** hoáº·c **"no"** cho tá»«ng nhÃ£n.

### CÃ¡c nhÃ£n phÃ¢n loáº¡i

Project há»— trá»£ 8 nhÃ£n:
- `antagonize`: ThÃ¡i Ä‘á»™ Ä‘á»‘i khÃ¡ng
- `condescending`: Coi thÆ°á»ng
- `dismissive`: Thá» Æ¡, bÃ¡c bá»
- `generalisation`: KhÃ¡i quÃ¡t hÃ³a
- `generalisation_unfair`: KhÃ¡i quÃ¡t hÃ³a khÃ´ng cÃ´ng báº±ng
- `healthy`: LÃ nh máº¡nh
- `hostile`: ThÃ¹ Ä‘á»‹ch
- `sarcastic`: ChÃ¢m biáº¿m

---

## ğŸ“ Cáº¥u trÃºc thÆ° má»¥c

```
SoftPromt/
â”‚
â”œâ”€â”€ README.md              # TÃ i liá»‡u hÆ°á»›ng dáº«n
â”œâ”€â”€ requirements.txt       # CÃ¡c thÆ° viá»‡n cáº§n thiáº¿t
â”‚
â”œâ”€â”€ data/                  # ThÆ° má»¥c chá»©a dá»¯ liá»‡u
â”‚   â”œâ”€â”€ train.csv          # Dá»¯ liá»‡u training
â”‚   â”œâ”€â”€ val.csv            # Dá»¯ liá»‡u validation
â”‚   â””â”€â”€ test.csv           # Dá»¯ liá»‡u test
â”‚
â””â”€â”€ src/                   # ThÆ° má»¥c chá»©a source code
    â”œâ”€â”€ config.py          # Cáº¥u hÃ¬nh model vÃ  training
    â””â”€â”€ train.py           # Script huáº¥n luyá»‡n chÃ­nh
```

### MÃ´ táº£ files

- **data/**: Chá»©a file CSV vá»›i cá»™t `comment` (vÄƒn báº£n) vÃ  cÃ¡c cá»™t nhÃ£n (0/1)
- **src/config.py**: Äá»‹nh nghÄ©a Ä‘Æ°á»ng dáº«n dá»¯ liá»‡u, tham sá»‘ model, vÃ  hyperparameters
- **src/train.py**: Chá»©a class `T5PromptLabelTrainer` vÃ  logic training

---

## âš™ï¸ CÃ i Ä‘áº·t

### 1. Táº¡o mÃ´i trÆ°á»ng áº£o (khuyáº¿n nghá»‹)

```bash
python -m venv venv
venv\Scripts\activate    # Windows
# source venv/bin/activate  # Linux/Mac
```

### 2. CÃ i Ä‘áº·t cÃ¡c thÆ° viá»‡n

```bash
pip install -r requirements.txt
```

CÃ¡c thÆ° viá»‡n chÃ­nh:
- `transformers`: Framework Hugging Face cho T5
- `peft`: ThÆ° viá»‡n Parameter-Efficient Fine-Tuning
- `datasets`: Xá»­ lÃ½ dá»¯ liá»‡u
- `torch`: PyTorch backend
- `scikit-learn`: TÃ­nh toÃ¡n metrics (AUC-ROC)
- `pandas`: Äá»c vÃ  xá»­ lÃ½ CSV

---

## ğŸš€ Cháº¡y Training

### Huáº¥n luyá»‡n má»™t nhÃ£n (single label)

Sá»­ dá»¥ng tham sá»‘ `--label` Ä‘á»ƒ chá»‰ huáº¥n luyá»‡n cho má»™t nhÃ£n cá»¥ thá»ƒ:

```bash
python src/train.py --label hostile
```

VÃ­ dá»¥ trÃªn sáº½ chá»‰ train model Ä‘á»ƒ phÃ¢n loáº¡i nhÃ£n **hostile**.

### Huáº¥n luyá»‡n nhiá»u nhÃ£n (multiple labels)

Sá»­ dá»¥ng tham sá»‘ `--labels` (sá»‘ nhiá»u) Ä‘á»ƒ huáº¥n luyá»‡n nhiá»u nhÃ£n cÃ¹ng lÃºc:

```bash
python src/train.py --labels hostile sarcastic antagonize
```

### Huáº¥n luyá»‡n táº¥t cáº£ cÃ¡c nhÃ£n

KhÃ´ng truyá»n `--label` hoáº·c `--labels` Ä‘á»ƒ train táº¥t cáº£ 8 nhÃ£n:

```bash
python src/train.py
```

### TÃ¹y chá»‰nh hyperparameters

Báº¡n cÃ³ thá»ƒ override cÃ¡c tham sá»‘ máº·c Ä‘á»‹nh:

```bash
python src/train.py --label healthy --epochs 10 --lr 0.001 --train_bs 32 --num_virtual_tokens 20
```

**CÃ¡c tham sá»‘ cÃ³ thá»ƒ tÃ¹y chá»‰nh:**
- `--epochs`: Sá»‘ epoch huáº¥n luyá»‡n (máº·c Ä‘á»‹nh: 5)
- `--lr`: Learning rate (máº·c Ä‘á»‹nh: 0.005)
- `--train_bs`: Batch size cho training (máº·c Ä‘á»‹nh: 16)
- `--num_virtual_tokens`: Sá»‘ lÆ°á»£ng virtual tokens (máº·c Ä‘á»‹nh: 10)
- `--fp16`: Sá»­ dá»¥ng mixed precision training
- `--no_eval_each_epoch`: Táº¯t evaluation sau má»—i epoch

### VÃ­ dá»¥ Ä‘áº§y Ä‘á»§

```bash
# Train 2 nhÃ£n vá»›i cáº¥u hÃ¬nh tÃ¹y chá»‰nh
python src/train.py --labels dismissive condescending --epochs 8 --lr 0.003 --train_bs 24 --fp16
```

---

## ğŸ“Š Káº¿t quáº£ Training

Trong quÃ¡ trÃ¬nh training, báº¡n sáº½ tháº¥y:
- Loss vÃ  learning rate sau má»—i logging step
- AUC-ROC score cho tá»«ng nhÃ£n sau má»—i epoch
- AUC-ROC macro (trung bÃ¬nh) trÃªn validation set
- ÄÃ¡nh giÃ¡ trÃªn test set trÆ°á»›c vÃ  sau khi train

Model adapter (chá»‰ virtual tokens) sáº½ Ä‘Æ°á»£c lÆ°u táº¡i thÆ° má»¥c Ä‘Æ°á»£c chá»‰ Ä‘á»‹nh trong `config.py`:
- Máº·c Ä‘á»‹nh: `./t5_prompt_adapter_selected_labels`

---

## ğŸ’¡ LÆ°u Ã½

1. **Dá»¯ liá»‡u**: File CSV pháº£i cÃ³ cá»™t `comment` vÃ  cÃ¡c cá»™t nhÃ£n vá»›i giÃ¡ trá»‹ 0 hoáº·c 1
2. **GPU**: Khuyáº¿n nghá»‹ sá»­ dá»¥ng GPU Ä‘á»ƒ tÄƒng tá»‘c Ä‘á»™ training
3. **Memory**: Prompt tuning tiáº¿t kiá»‡m bá»™ nhá»› hÆ¡n nhiá»u so vá»›i full fine-tuning
4. **Adapter**: Chá»‰ cáº§n lÆ°u vÃ  chia sáº» adapter (~KB) thay vÃ¬ toÃ n bá»™ model (~GB)

---

## ğŸ“ Tham kháº£o

- [PEFT Library](https://github.com/huggingface/peft)
- [T5 Model](https://huggingface.co/t5-base)
- [Prompt Tuning Paper](https://arxiv.org/abs/2104.08691)

